{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {'modules/components.py'}\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (\n",
    "    CsvExampleGen,\n",
    "    StatisticsGen,\n",
    "    SchemaGen,\n",
    "    ExampleValidator,\n",
    "    Transform,\n",
    "    Tuner,\n",
    "    Trainer,\n",
    "    Evaluator,\n",
    "    Pusher\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy\n",
    ")\n",
    "\n",
    "def init_components(\n",
    "        data_dir,\n",
    "        transform_module,\n",
    "        tuner_module,\n",
    "        training_module,\n",
    "        training_steps,\n",
    "        eval_steps,\n",
    "        serving_model_dir\n",
    "    ):\n",
    "    '''\n",
    "    Initiate TFX pipeline components.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to the data.\n",
    "        transform_module (str): Path to the transform module.\n",
    "        tuner_module (str): Path to the tuner module.\n",
    "        training_module (str): Path to the training module.\n",
    "        training_steps (int): Number of training steps.\n",
    "        eval_steps (int): Number of evaluation steps.\n",
    "        serving_model_dir (str): Path to the serving model directory.\n",
    "    \n",
    "    Returns:\n",
    "        TFX components.\n",
    "    '''\n",
    "\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    example_gen = CsvExampleGen(input_base=data_dir, output_config=output)\n",
    "\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(transform_module)\n",
    "    )\n",
    "\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(tuner_module),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=['train'],\n",
    "            num_steps=training_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=['eval'],\n",
    "            num_steps=eval_steps)\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=os.path.abspath(training_module),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=['train'],\n",
    "            num_steps=training_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=['eval'],\n",
    "            num_steps=eval_steps)\n",
    "    )\n",
    "    \n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing)\n",
    "    ).with_id('Latest_blessed_model_resolver')\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='diabetes')],\n",
    "        slicing_specs=[\n",
    "            tfma.SlicingSpec(),\n",
    "            tfma.SlicingSpec(feature_keys=['diabetes'])\n",
    "        ],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='AUC'),\n",
    "                tfma.MetricConfig(class_name='Precision'),\n",
    "                tfma.MetricConfig(class_name='Recall'),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.5}\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': 0.0001}\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    components = (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher\n",
    "    )\n",
    "\n",
    "    return components\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {'modules/transform.py'}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'high_bp',\n",
    "    'high_chol',\n",
    "    'chol_check',\n",
    "    'smoker',\n",
    "    'stroke',\n",
    "    'heart_disease_or_attack',\n",
    "    'phys_activity',\n",
    "    'fruits',\n",
    "    'veggies',\n",
    "    'hvy_alcohol_consump',\n",
    "    'any_healthcare',\n",
    "    'no_docbc_cost',\n",
    "    'diff_walk',\n",
    "    'sex'\n",
    "]\n",
    "\n",
    "NUMERICAL_FEATURES = [\n",
    "    'bmi',\n",
    "    'gen_hlth',\n",
    "    'ment_hlth',\n",
    "    'phys_hlth',\n",
    "    'age',\n",
    "    'education',\n",
    "    'income'\n",
    "]\n",
    "\n",
    "LABEL_KEY = 'diabetes'\n",
    "\n",
    "def transformed_name(key):\n",
    "    '''\n",
    "    Rename transformed features.\n",
    "\n",
    "    Args:\n",
    "        key (str): Feature name to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "        str: Transformed feature name.\n",
    "    '''\n",
    "\n",
    "    return key + '_xf'\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    '''\n",
    "    Preprocess input features into transformed features.\n",
    "    \n",
    "    Args:\n",
    "        inputs (dict): Map from feature keys to raw features.\n",
    "        \n",
    "    Returns:\n",
    "        outputs (dict): Map from feature keys to transformed features.\n",
    "    '''\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tf.cast(inputs[feature], tf.int64)\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_0_1(inputs[feature])\n",
    "\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "\n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {'modules/tuner.py'}\n",
    "\n",
    "from typing import NamedTuple, Dict, Text, Any, List\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    concatenate,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Dropout\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "import kerastuner as kt\n",
    "from keras_tuner.engine import base_tuner\n",
    "\n",
    "from modules.transform import (\n",
    "    CATEGORICAL_FEATURES,\n",
    "    NUMERICAL_FEATURES,\n",
    "    LABEL_KEY,\n",
    "    transformed_name\n",
    ")\n",
    "\n",
    "# Define the result type for the tuner function.\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [\n",
    "    ('tuner', base_tuner.BaseTuner), \n",
    "    ('fit_kwargs', Dict[Text, Any])\n",
    "])\n",
    "\n",
    "# Configure early stopping to prevent overfitting.\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_binary_accuracy', \n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    '''Loads compressed data.'''\n",
    "    \n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "def input_fn(\n",
    "        file_pattern,\n",
    "        tf_transform_output,\n",
    "        batch_size=64\n",
    "    ) -> tf.data.Dataset:\n",
    "    '''\n",
    "    Generates features and labels for tuning/training.\n",
    "    \n",
    "    Args:\n",
    "        file_pattern (str): Input tfrecord file pattern.\n",
    "        tf_transform_output: A TFTransformOutput.\n",
    "        batch_size (int): Number of consecutive elements of returned dataset.\n",
    "\n",
    "    Returns:\n",
    "        A dataset that provides (features, label) tuples.\n",
    "    '''\n",
    "\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tuner_model(hp):\n",
    "    '''\n",
    "    Builds the model for hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        hp: Hyperparameters object.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model.\n",
    "    '''\n",
    "\n",
    "    # Define input layers.\n",
    "    input_features = []\n",
    "\n",
    "    for feature in CATEGORICAL_FEATURES + NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    # Concatenate inputs and pass through Sequential layers.\n",
    "    concatenated = concatenate(input_features)\n",
    "\n",
    "    # Define dense layers as a Sequential model.\n",
    "    dense_layers = tf.keras.models.Sequential([\n",
    "        Dense(\n",
    "            hp.Choice('dense_units', values=[128, 256, 512]), \n",
    "            activation='relu'\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='dense_stack')\n",
    "    \n",
    "    outputs = dense_layers(concatenated)\n",
    "\n",
    "    # Build and compile model.\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    '''\n",
    "    Tune the hyperparameters for the model.\n",
    "\n",
    "    Args:\n",
    "        fn_args: Holds args as name/value pairs.\n",
    "    \n",
    "    Returns:\n",
    "        A namedtuple contains the tuner and fit_kwargs.\n",
    "    '''\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Prepare training and validation datasets.\n",
    "    train_set = input_fn(fn_args.train_files[0], tf_transform_output)\n",
    "    val_set = input_fn(fn_args.eval_files[0], tf_transform_output)\n",
    "\n",
    "    # Initialize the tuner.\n",
    "    tuner = kt.Hyperband(\n",
    "        tuner_model,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='diabetes_kt_hyperband'\n",
    "    )\n",
    "\n",
    "    # Return the tuner and fit arguments.\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            'x': train_set,\n",
    "            'validation_data': val_set,\n",
    "            'steps_per_epoch': 1000,\n",
    "            'validation_steps': 1000,\n",
    "            'callbacks': [early_stopping]\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {'modules/trainer.py'}\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    concatenate,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Dropout\n",
    ")\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from modules.transform import (\n",
    "    CATEGORICAL_FEATURES,\n",
    "    NUMERICAL_FEATURES,\n",
    "    LABEL_KEY,\n",
    "    transformed_name\n",
    ")\n",
    "from modules.tuner import gzip_reader_fn, input_fn\n",
    "\n",
    "def trainer_model(hp):\n",
    "    '''\n",
    "    Builds the model for training.\n",
    "\n",
    "    Args:\n",
    "        hp: Hyperparameters object.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model.\n",
    "    '''\n",
    "\n",
    "    # Define input layers.\n",
    "    input_features = []\n",
    "\n",
    "    for feature in CATEGORICAL_FEATURES + NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    # Concatenate inputs and pass through Sequential layers.\n",
    "    concatenated = concatenate(input_features)\n",
    "\n",
    "    # Define dense layers as a Sequential model.\n",
    "    dense_layers = tf.keras.models.Sequential([\n",
    "        Dense(hp['dense_units'], activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(hp['dropout_rate']),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='dense_stack')\n",
    "    \n",
    "    outputs = dense_layers(concatenated)\n",
    "\n",
    "    # Build and compile model.\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(hp['learning_rate']),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    '''Returns a function that parses a serialized tf.Example.'''\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        '''Returns the output to be used in the serving signature.'''\n",
    "\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec\n",
    "        )\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        outputs = model(transformed_features)\n",
    "\n",
    "        return {'outputs': outputs}\n",
    "    \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def run_fn(fn_args) -> None:\n",
    "    '''\n",
    "    Train the model based on given args.\n",
    "    \n",
    "    Args:\n",
    "        fn_args: Holds args used to train the model as name/value pairs.\n",
    "    '''\n",
    "\n",
    "    # Load the transform output.\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    # Prepare training and validation datasets.\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output)\n",
    "\n",
    "    # Build model.\n",
    "    hp = fn_args.hyperparameters['values']\n",
    "    model = trainer_model(hp)\n",
    "\n",
    "    # Define callbacks.\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    tensorboard = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_binary_accuracy', \n",
    "        patience=3, \n",
    "        verbose=1, \n",
    "        mode='max', \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        fn_args.serving_model_dir,\n",
    "        monitor='val_binary_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    # Train model.\n",
    "    model.fit(\n",
    "        train_set,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard, early_stopping, model_checkpoint],\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Define and save model signatures for serving\n",
    "    signatures = {\n",
    "        'serving_default': get_serve_tf_examples_fn(\n",
    "            model, tf_transform_output\n",
    "        ).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "        )\n",
    "    }\n",
    "\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format='tf', signatures=signatures\n",
    "    )\n",
    "\n",
    "    # Save model plot.\n",
    "    plot_model(\n",
    "        model, \n",
    "        to_file='images/model_plot.png', \n",
    "        show_shapes=True, \n",
    "        show_layer_names=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {'local_pipeline.py'}\n",
    "\n",
    "import os\n",
    "from typing import Text\n",
    "\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "\n",
    "PIPELINE_NAME = 'diabetes-pipeline'\n",
    "\n",
    "# Pipeline inputs.\n",
    "DATA_ROOT = 'data'\n",
    "TRANSFORM_MODULE_FILE = 'modules/transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/tuner.py'\n",
    "TRAINER_MODULE_FILE = 'modules/trainer.py'\n",
    "\n",
    "# Pipeline outputs.\n",
    "OUTPUT_BASE = 'output'\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')\n",
    "\n",
    "def init_local_pipeline(\n",
    "        components,\n",
    "        pipeline_root: Text\n",
    "    ) -> pipeline.Pipeline:\n",
    "\n",
    "    logging.info(f'Pipeline root set to: {pipeline_root}')\n",
    "    beam_args = [\n",
    "        '--direct_running_mode=multi_processing',\n",
    "        '--direct_num_workers=1'\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "\n",
    "    from modules.components import init_components\n",
    "\n",
    "    components = init_components(\n",
    "        data_dir=DATA_ROOT,\n",
    "        transform_module=TRANSFORM_MODULE_FILE,\n",
    "        tuner_module=TUNER_MODULE_FILE,\n",
    "        training_module=TRAINER_MODULE_FILE,\n",
    "        training_steps=10000,\n",
    "        eval_steps=1000,\n",
    "        serving_model_dir=serving_model_dir\n",
    "    )\n",
    "\n",
    "    pipeline = init_local_pipeline(components, pipeline_root)\n",
    "    BeamDagRunner().run(pipeline=pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-11 13:22:49.646381: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 13:22:49.757613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:49.757648: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-02-11 13:22:49.780289: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 13:22:50.432413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:50.432524: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:50.432550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Pipeline root set to: output/diabetes-pipeline\n",
      "INFO:absl:Generating ephemeral wheel package for '/workspaces/indonesian-sentiment-analysis/modules/transform.py' (including modules: ['tuner', 'transform', 'components', 'trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.\n",
      "INFO:absl:Executing: ['/opt/conda/envs/diabetes-prediction/bin/python', '/tmp/tmpvec53id_/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpr6thl3ca', '--dist-dir', '/tmp/tmpqjtbaf96']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying tuner.py -> build/lib\n",
      "copying transform.py -> build/lib\n",
      "copying components.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /tmp/tmpr6thl3ca\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transform.py -> /tmp/tmpr6thl3ca\n",
      "copying build/lib/components.py -> /tmp/tmpr6thl3ca\n",
      "copying build/lib/tuner.py -> /tmp/tmpr6thl3ca\n",
      "copying build/lib/trainer.py -> /tmp/tmpr6thl3ca\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpr6thl3ca/tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpr6thl3ca/tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL\n",
      "creating '/tmp/tmpqjtbaf96/tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl' and adding '/tmp/tmpr6thl3ca' to it\n",
      "adding 'components.py'\n",
      "adding 'trainer.py'\n",
      "adding 'transform.py'\n",
      "adding 'tuner.py'\n",
      "adding 'tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/RECORD'\n",
      "removing /tmp/tmpr6thl3ca\n",
      "INFO:absl:Successfully built user code wheel distribution at 'output/diabetes-pipeline/_wheels/tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'; target user module is 'transform'.\n",
      "INFO:absl:Full user module path is 'transform@output/diabetes-pipeline/_wheels/tfx_user_code_Transform-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/workspaces/indonesian-sentiment-analysis/modules/tuner.py' (including modules: ['tuner', 'transform', 'components', 'trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.\n",
      "INFO:absl:Executing: ['/opt/conda/envs/diabetes-prediction/bin/python', '/tmp/tmpxdyqb9xu/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpehrl60ao', '--dist-dir', '/tmp/tmpa222an9k']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying tuner.py -> build/lib\n",
      "copying transform.py -> build/lib\n",
      "copying components.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /tmp/tmpehrl60ao\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transform.py -> /tmp/tmpehrl60ao\n",
      "copying build/lib/components.py -> /tmp/tmpehrl60ao\n",
      "copying build/lib/tuner.py -> /tmp/tmpehrl60ao\n",
      "copying build/lib/trainer.py -> /tmp/tmpehrl60ao\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Tuner.egg-info\n",
      "writing tfx_user_code_Tuner.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Tuner.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Tuner.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Tuner.egg-info to /tmp/tmpehrl60ao/tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpehrl60ao/tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL\n",
      "creating '/tmp/tmpa222an9k/tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl' and adding '/tmp/tmpehrl60ao' to it\n",
      "adding 'components.py'\n",
      "adding 'trainer.py'\n",
      "adding 'transform.py'\n",
      "adding 'tuner.py'\n",
      "adding 'tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/RECORD'\n",
      "removing /tmp/tmpehrl60ao\n",
      "INFO:absl:Successfully built user code wheel distribution at 'output/diabetes-pipeline/_wheels/tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'; target user module is 'tuner'.\n",
      "INFO:absl:Full user module path is 'tuner@output/diabetes-pipeline/_wheels/tfx_user_code_Tuner-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/workspaces/indonesian-sentiment-analysis/modules/trainer.py' (including modules: ['tuner', 'transform', 'components', 'trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.\n",
      "INFO:absl:Executing: ['/opt/conda/envs/diabetes-prediction/bin/python', '/tmp/tmp0o23xnhh/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpmaqhri3v', '--dist-dir', '/tmp/tmpkf45ngqv']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying tuner.py -> build/lib\n",
      "copying transform.py -> build/lib\n",
      "copying components.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /tmp/tmpmaqhri3v\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transform.py -> /tmp/tmpmaqhri3v\n",
      "copying build/lib/components.py -> /tmp/tmpmaqhri3v\n",
      "copying build/lib/tuner.py -> /tmp/tmpmaqhri3v\n",
      "copying build/lib/trainer.py -> /tmp/tmpmaqhri3v\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpmaqhri3v/tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpmaqhri3v/tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL\n",
      "creating '/tmp/tmpkf45ngqv/tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl' and adding '/tmp/tmpmaqhri3v' to it\n",
      "adding 'components.py'\n",
      "adding 'trainer.py'\n",
      "adding 'transform.py'\n",
      "adding 'tuner.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358.dist-info/RECORD'\n",
      "removing /tmp/tmpmaqhri3v\n",
      "INFO:absl:Successfully built user code wheel distribution at 'output/diabetes-pipeline/_wheels/tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'; target user module is 'trainer'.\n",
      "INFO:absl:Full user module path is 'trainer@output/diabetes-pipeline/_wheels/tfx_user_code_Trainer-0.0+31e82c0b1d4caf4600f17d8edd66a620e89990a34e1c93397a65013f0f3ff358-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=2\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=2\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"SchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=2\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Transform\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=2\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=2\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Tuner\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.tuner.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"output/diabetes-pipeline/metadata.sqlite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"output/diabetes-pipeline/metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Node CsvExampleGen depends on [].\n",
      "INFO:absl:Node CsvExampleGen is scheduled.\n",
      "INFO:absl:Node Latest_blessed_model_resolver depends on [].\n",
      "INFO:absl:Node Latest_blessed_model_resolver is scheduled.\n",
      "INFO:absl:Node StatisticsGen depends on ['Run[CsvExampleGen]'].\n",
      "INFO:absl:Node StatisticsGen is scheduled.\n",
      "INFO:absl:Node SchemaGen depends on ['Run[StatisticsGen]'].\n",
      "INFO:absl:Node SchemaGen is scheduled.\n",
      "INFO:absl:Node ExampleValidator depends on ['Run[SchemaGen]', 'Run[StatisticsGen]'].\n",
      "INFO:absl:Node ExampleValidator is scheduled.\n",
      "INFO:absl:Node Transform depends on ['Run[CsvExampleGen]', 'Run[SchemaGen]'].\n",
      "INFO:absl:Node Transform is scheduled.\n",
      "INFO:absl:Node Tuner depends on ['Run[SchemaGen]', 'Run[Transform]'].\n",
      "INFO:absl:Node Tuner is scheduled.\n",
      "INFO:absl:Node Trainer depends on ['Run[SchemaGen]', 'Run[Transform]', 'Run[Tuner]'].\n",
      "INFO:absl:Node Trainer is scheduled.\n",
      "INFO:absl:Node Evaluator depends on ['Run[CsvExampleGen]', 'Run[Latest_blessed_model_resolver]', 'Run[Trainer]'].\n",
      "INFO:absl:Node Evaluator is scheduled.\n",
      "INFO:absl:Node Pusher depends on ['Run[Evaluator]', 'Run[Trainer]'].\n",
      "INFO:absl:Node Pusher is scheduled.\n",
      "INFO:absl:node CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250211-132253.873494\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:[CsvExampleGen] Resolved inputs: ({},)\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 1\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"output/diabetes-pipeline/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:6347579,xor_checksum:1739273022,sum_checksum:1739273022\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_base': 'data', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 8,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:6347579,xor_checksum:1739273022,sum_checksum:1739273022'}, execution_output_uri='output/diabetes-pipeline/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='output/diabetes-pipeline/CsvExampleGen/.system/stateful_working_dir/20250211-132253.873494', tmp_dir='output/diabetes-pipeline/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250211-132253.873494\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diabetes-pipeline\"\n",
      ", pipeline_run_id='20250211-132253.873494')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir /opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx to temp dir /tmp/tmpfdtvcge7/build/tfx\n",
      "INFO:absl:Generating a temp setup file at /tmp/tmpfdtvcge7/build/tfx/setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpfdtvcge7/build/tfx/setup.log\n",
      "INFO:absl:Added --extra_package=/tmp/tmpfdtvcge7/build/tfx/dist/tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data data/* to TFExample.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280176   nanos: 897883176 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker_main.py:332\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280176   nanos: 900677919 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/options/pipeline_options.py:352\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280176   nanos: 984080076 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker_main.py:332\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280176   nanos: 987226009 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/options/pipeline_options.py:352\" thread: \"MainThread\" \n",
      "2025-02-11 13:22:57.125575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 13:22:57.126313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 13:22:57.232531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.232531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.232564: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-02-11 13:22:57.232569: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-02-11 13:22:57.261235: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 13:22:57.261406: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 13:22:57.826832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.826962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.826987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-02-11 13:22:57.847668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.847763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:22:57.847780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280191   nanos: 563199281 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_14\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280191   nanos: 572166681 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_15\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 1 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"output/diabetes-pipeline/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:6347579,xor_checksum:1739273022,sum_checksum:1739273022\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 1\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node CsvExampleGen is finished.\n",
      "INFO:absl:node Latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"Latest_blessed_model_resolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250211-132253.873494\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline.Latest_blessed_model_resolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"_generated_model_3\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"_generated_modelblessing_4\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model_blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input_graphs {\n",
      "    key: \"graph_1\"\n",
      "    value {\n",
      "      nodes {\n",
      "        key: \"dict_2\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          dict_node {\n",
      "            node_ids {\n",
      "              key: \"model\"\n",
      "              value: \"input_3\"\n",
      "            }\n",
      "            node_ids {\n",
      "              key: \"model_blessing\"\n",
      "              value: \"input_4\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_3\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_model_3\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_4\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_modelblessing_4\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"op_1\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          op_node {\n",
      "            op_type: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "            args {\n",
      "              node_id: \"dict_2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      result_node: \"op_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an resolver node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:[Latest_blessed_model_resolver] Resolved inputs: ({'model': [], 'model_blessing': []},)\n",
      "INFO:absl:node Latest_blessed_model_resolver is finished.\n",
      "INFO:absl:node StatisticsGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250211-132253.873494\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250211-132253.873494\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"output/diabetes-pipeline/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:6347579,xor_checksum:1739273022,sum_checksum:1739273022\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1739280194682\n",
      "last_update_time_since_epoch: 1739280194682\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 3\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"output/diabetes-pipeline/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:6347579,xor_checksum:1739273022,sum_checksum:1739273022\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1739280194682\n",
      "last_update_time_since_epoch: 1739280194682\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"output/diabetes-pipeline/StatisticsGen/statistics/3\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='output/diabetes-pipeline/StatisticsGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='output/diabetes-pipeline/StatisticsGen/.system/stateful_working_dir/20250211-132253.873494', tmp_dir='output/diabetes-pipeline/StatisticsGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250211-132253.873494\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diabetes-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250211-132253.873494\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diabetes-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diabetes-pipeline\"\n",
      ", pipeline_run_id='20250211-132253.873494')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir /opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx to temp dir /tmp/tmpl4coyr47/build/tfx\n",
      "INFO:absl:Generating a temp setup file at /tmp/tmpl4coyr47/build/tfx/setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpl4coyr47/build/tfx/setup.log\n",
      "INFO:absl:Added --extra_package=/tmp/tmpl4coyr47/build/tfx/dist/tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to output/diabetes-pipeline/StatisticsGen/statistics/3/Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to output/diabetes-pipeline/StatisticsGen/statistics/3/Split-eval.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280198   nanos: 822982072 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker_main.py:332\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280198   nanos: 825887441 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/options/pipeline_options.py:352\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280198   nanos: 827991485 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker_main.py:332\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280198   nanos: 831638097 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/options/pipeline_options.py:352\" thread: \"MainThread\" \n",
      "2025-02-11 13:23:18.992389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 13:23:18.993809: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 13:23:19.102945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.102945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.102983: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-02-11 13:23:19.102983: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-02-11 13:23:19.131177: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 13:23:19.131324: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 13:23:19.679831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.679966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.679988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-02-11 13:23:19.682002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.682100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-11 13:23:19.682122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280200   nanos: 988422632 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_44\" transform_id: \"TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1739280200   nanos: 993232727 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_43\" transform_id: \"TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "ERROR:absl:Execution 3 failed.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 983, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/beam/beam_dag_runner.py\", line 90, in process\n",
      "    self._run_node()\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/beam/beam_dag_runner.py\", line 99, in _run_node\n",
      "    launcher.Launcher(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py\", line 573, in launch\n",
      "    executor_output = self._run_executor(execution_info)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py\", line 448, in _run_executor\n",
      "    executor_output = self._executor_operator.run_executor(execution_info)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/beam_executor_operator.py\", line 112, in run_executor\n",
      "    return python_executor_operator.run_with_executor(execution_info, executor)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/python_executor_operator.py\", line 58, in run_with_executor\n",
      "    result = executor.Do(execution_info.input_dict, output_dict,\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/components/statistics_gen/executor.py\", line 161, in Do\n",
      "    logging.info('Statistics for split %s written to %s.', split,\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
      "    self.result = self.run()\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 577, in run\n",
      "    return self.runner.run_pipeline(self, self._options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/direct/direct_runner.py\", line 131, in run_pipeline\n",
      "    return runner.run_pipeline(pipeline, options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 201, in run_pipeline\n",
      "    self._latest_run_result = self.run_via_runner_api(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 222, in run_via_runner_api\n",
      "    return self.run_stages(stage_context, stages)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 453, in run_stages\n",
      "    bundle_results = self._execute_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 781, in _execute_bundle\n",
      "    self._run_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1010, in _run_bundle\n",
      "    result, splits = bundle_manager.process_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1430, in process_bundle\n",
      "    for ix, part in enumerate(input.partition(self._num_workers)):\n",
      "AttributeError: 'NoneType' object has no attribute 'partition'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/indonesian-sentiment-analysis/local_pipeline.py\", line 61, in <module>\n",
      "    BeamDagRunner().run(pipeline=pipeline)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/tfx_runner.py\", line 124, in run\n",
      "    return self.run_with_ir(pipeline_pb, run_options=run_options_pb, **kwargs)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/beam/beam_dag_runner.py\", line 297, in run_with_ir\n",
      "    logging.info('Node %s is scheduled.', node_id)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
      "    self.result = self.run()\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 577, in run\n",
      "    return self.runner.run_pipeline(self, self._options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/direct/direct_runner.py\", line 131, in run_pipeline\n",
      "    return runner.run_pipeline(pipeline, options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 201, in run_pipeline\n",
      "    self._latest_run_result = self.run_via_runner_api(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 222, in run_via_runner_api\n",
      "    return self.run_stages(stage_context, stages)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 453, in run_stages\n",
      "    bundle_results = self._execute_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 781, in _execute_bundle\n",
      "    self._run_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1010, in _run_bundle\n",
      "    result, splits = bundle_manager.process_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1346, in process_bundle\n",
      "    result_future = self._worker_handler.control_conn.push(process_bundle_req)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\", line 379, in push\n",
      "    response = self.worker.do_instruction(request)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 596, in do_instruction\n",
      "    return getattr(self, request_type)(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 634, in process_bundle\n",
      "    bundle_processor.process_bundle(instruction_id))\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1003, in process_bundle\n",
      "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 227, in process_encoded\n",
      "    self.output(decoded_value)\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 526, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 528, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 1507, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"apache_beam/runners/common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 983, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/beam/beam_dag_runner.py\", line 90, in process\n",
      "    self._run_node()\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/beam/beam_dag_runner.py\", line 99, in _run_node\n",
      "    launcher.Launcher(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py\", line 573, in launch\n",
      "    executor_output = self._run_executor(execution_info)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py\", line 448, in _run_executor\n",
      "    executor_output = self._executor_operator.run_executor(execution_info)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/beam_executor_operator.py\", line 112, in run_executor\n",
      "    return python_executor_operator.run_with_executor(execution_info, executor)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/orchestration/portable/python_executor_operator.py\", line 58, in run_with_executor\n",
      "    result = executor.Do(execution_info.input_dict, output_dict,\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/tfx/components/statistics_gen/executor.py\", line 161, in Do\n",
      "    logging.info('Statistics for split %s written to %s.', split,\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
      "    self.result = self.run()\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/pipeline.py\", line 577, in run\n",
      "    return self.runner.run_pipeline(self, self._options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/direct/direct_runner.py\", line 131, in run_pipeline\n",
      "    return runner.run_pipeline(pipeline, options)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 201, in run_pipeline\n",
      "    self._latest_run_result = self.run_via_runner_api(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 222, in run_via_runner_api\n",
      "    return self.run_stages(stage_context, stages)\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 453, in run_stages\n",
      "    bundle_results = self._execute_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 781, in _execute_bundle\n",
      "    self._run_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1010, in _run_bundle\n",
      "    result, splits = bundle_manager.process_bundle(\n",
      "  File \"/opt/conda/envs/diabetes-prediction/lib/python3.9/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1430, in process_bundle\n",
      "    for ix, part in enumerate(input.partition(self._num_workers)):\n",
      "AttributeError: 'NoneType' object has no attribute 'partition' [while running 'Run[StatisticsGen]']\n"
     ]
    }
   ],
   "source": [
    "!python local_pipeline.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
